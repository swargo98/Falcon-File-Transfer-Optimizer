{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<a href=\"https://colab.research.google.com/github/swargo98/Falcon-File-Transfer-Optimizer/blob/master/Network_Simulator_PPO_128.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T22:12:25.242407Z",
     "start_time": "2024-12-04T22:12:25.240511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Important params: k, max_steps, episodeds, action space, randomization of state, entropy?\n",
    "# try changing std with covariance matrix"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T22:12:26.434480Z",
     "start_time": "2024-12-04T22:12:25.286180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from queue import PriorityQueue"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T22:12:26.529734Z",
     "start_time": "2024-12-04T22:12:26.489690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T22:12:26.541069Z",
     "start_time": "2024-12-04T22:12:26.535921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_model(agent, filename_policy, filename_value):\n",
    "    torch.save(agent.policy.state_dict(), filename_policy)\n",
    "    torch.save(agent.value_function.state_dict(), filename_value)\n",
    "    print(\"Model saved successfully.\")\n",
    "\n",
    "\n",
    "def load_model(agent, filename_policy, filename_value):\n",
    "    agent.policy.load_state_dict(torch.load(filename_policy))\n",
    "    agent.policy_old.load_state_dict(agent.policy.state_dict())\n",
    "    agent.value_function.load_state_dict(torch.load(filename_value))\n",
    "    print(\"Model loaded successfully.\")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T22:12:26.585378Z",
     "start_time": "2024-12-04T22:12:26.582143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimulatorState:\n",
    "    def __init__(self, sender_buffer_remaining_capacity=0, receiver_buffer_remaining_capacity=0,\n",
    "                 read_throughput=0, write_throughput=0, network_throughput=0,\n",
    "                 read_thread=0, write_thread=0, network_thread=0) -> None:\n",
    "        self.sender_buffer_remaining_capacity = sender_buffer_remaining_capacity\n",
    "        self.receiver_buffer_remaining_capacity = receiver_buffer_remaining_capacity\n",
    "        self.read_throughput = read_throughput\n",
    "        self.write_throughput = write_throughput\n",
    "        self.network_throughput = network_throughput\n",
    "        self.read_thread = read_thread\n",
    "        self.write_thread = write_thread\n",
    "        self.network_thread = network_thread\n",
    "\n",
    "    def copy(self):\n",
    "        # Return a new SimulatorState instance with the same attribute values\n",
    "        return SimulatorState(\n",
    "            sender_buffer_remaining_capacity=self.sender_buffer_remaining_capacity,\n",
    "            receiver_buffer_remaining_capacity=self.receiver_buffer_remaining_capacity,\n",
    "            read_throughput=self.read_throughput,\n",
    "            write_throughput=self.write_throughput,\n",
    "            network_throughput=self.network_throughput,\n",
    "            read_thread=self.read_thread,\n",
    "            write_thread=self.write_thread,\n",
    "            network_thread=self.network_thread\n",
    "        )\n",
    "\n",
    "    def to_array(self):\n",
    "        # Convert the state to a NumPy array\n",
    "        return np.array([\n",
    "            self.sender_buffer_remaining_capacity,\n",
    "            self.receiver_buffer_remaining_capacity,\n",
    "            self.read_throughput,\n",
    "            self.write_throughput,\n",
    "            self.network_throughput,\n",
    "            self.read_thread,\n",
    "            self.write_thread,\n",
    "            self.network_thread\n",
    "        ], dtype=np.float32)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T22:12:26.641923Z",
     "start_time": "2024-12-04T22:12:26.633103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing_extensions import final\n",
    "class NetworkSystemSimulator:\n",
    "    def __init__(self, read_thread = 1, network_thread = 1, write_thread = 1, sender_buffer_capacity = 10, receiver_buffer_capacity = 10, read_throughput_per_thread = 3, write_throughput_per_thread = 1, network_throughput_per_thread = 2, read_bandwidth = 6, write_bandwidth = 6, network_bandwidth = 6, read_background_traffic = 0, write_background_traffic = 0, network_background_traffic = 0, track_states = False):\n",
    "        self.sender_buffer_capacity = sender_buffer_capacity\n",
    "        self.receiver_buffer_capacity = receiver_buffer_capacity\n",
    "        self.read_throughput_per_thread = read_throughput_per_thread\n",
    "        self.write_throughput_per_thread = write_throughput_per_thread\n",
    "        self.network_throughput_per_thread = network_throughput_per_thread\n",
    "        self.read_bandwidth = read_bandwidth\n",
    "        self.write_bandwidth = write_bandwidth\n",
    "        self.network_bandwidth = network_bandwidth\n",
    "        self.read_background_traffic = read_background_traffic\n",
    "        self.write_background_traffic = write_background_traffic\n",
    "        self.network_background_traffic = network_background_traffic\n",
    "        self.read_thread = read_thread\n",
    "        self.network_thread = network_thread\n",
    "        self.write_thread = write_thread\n",
    "        self.track_states = track_states\n",
    "        self.K = 1.02\n",
    "\n",
    "        # Initialize the buffers\n",
    "        self.sender_buffer_in_use = max(min(self.read_throughput_per_thread * read_thread - self.network_throughput_per_thread * self.network_thread, self.sender_buffer_capacity), 0)\n",
    "        self.receiver_buffer_in_use = max(min(self.network_throughput_per_thread * network_thread - self.write_throughput_per_thread * self.write_thread, self.receiver_buffer_capacity), 0)\n",
    "\n",
    "        print(f\"Initial Sender Buffer: {self.sender_buffer_in_use}, Receiver Buffer: {self.receiver_buffer_in_use}\")\n",
    "\n",
    "\n",
    "        if self.track_states:\n",
    "            with open('optimizer_call_level_states.csv', 'w') as f:\n",
    "                f.write(\"Read Thread, Network Thread, Write Thread, Utility, Read Throughput, Sender Buffer, Network Throughput, Receiver Buffer, Write Throughput\\n\")\n",
    "\n",
    "            with open('thread_level_states.csv', 'w') as f:\n",
    "                f.write(\"Thread Type, Throughput, Sender Buffer, Receiver Buffer\\n\")\n",
    "                f.write(f\"Initial, 0, {self.sender_buffer_in_use}, {self.receiver_buffer_in_use}\\n\")\n",
    "\n",
    "    def read_thread_task(self, time):\n",
    "        throughput_increase = 0\n",
    "        if self.sender_buffer_in_use < self.sender_buffer_capacity:\n",
    "            read_throughput_temp = min(self.read_throughput_per_thread, self.sender_buffer_capacity - self.sender_buffer_in_use)\n",
    "            throughput_increase = min(read_throughput_temp, self.read_bandwidth-self.read_throughput)\n",
    "            self.read_throughput += throughput_increase\n",
    "            self.sender_buffer_in_use += throughput_increase\n",
    "\n",
    "        time_taken = throughput_increase / self.read_throughput_per_thread\n",
    "        next_time = time + time_taken + 0.001\n",
    "        if next_time < 1:\n",
    "            self.thread_queue.put((next_time, \"read\"))\n",
    "\n",
    "        if throughput_increase > 0 and self.track_states:\n",
    "            with open('thread_level_states.csv', 'a') as f:\n",
    "                f.write(f\"Read, {throughput_increase}, {self.sender_buffer_in_use}, {self.receiver_buffer_in_use}\\n\")\n",
    "        return next_time\n",
    "\n",
    "    def network_thread_task(self, time):\n",
    "        throughput_increase = 0\n",
    "        # print(f\"Network Thread start: Network Throughput: {throughput_increase}, Sender Buffer: {self.sender_buffer_in_use}, Receiver Buffer: {self.receiver_buffer_in_use}\")\n",
    "        if self.sender_buffer_in_use > 0 and self.receiver_buffer_in_use < self.receiver_buffer_capacity:\n",
    "            network_throughput_temp = min(self.network_throughput_per_thread, self.sender_buffer_in_use, self.receiver_buffer_capacity - self.receiver_buffer_in_use)\n",
    "            throughput_increase = min(network_throughput_temp, self.network_bandwidth-self.network_throughput)\n",
    "            self.network_throughput += throughput_increase\n",
    "            self.sender_buffer_in_use -= throughput_increase\n",
    "            self.receiver_buffer_in_use += throughput_increase\n",
    "\n",
    "        time_taken = throughput_increase / self.network_throughput_per_thread\n",
    "        next_time = time + time_taken + 0.001\n",
    "        if next_time < 1:\n",
    "            self.thread_queue.put((next_time, \"network\"))\n",
    "        # print(f\"Network Thread end: Network Throughput: {throughput_increase}, Sender Buffer: {self.sender_buffer_in_use}, Receiver Buffer: {self.receiver_buffer_in_use}\")\n",
    "        if throughput_increase > 0 and self.track_states:\n",
    "            with open('thread_level_states.csv', 'a') as f:\n",
    "                f.write(f\"Network, {throughput_increase}, {self.sender_buffer_in_use}, {self.receiver_buffer_in_use}\\n\")\n",
    "        return next_time\n",
    "\n",
    "    def write_thread_task(self, time):\n",
    "        throughput_increase = 0\n",
    "        if self.receiver_buffer_in_use > 0:\n",
    "            write_throughput_temp = min(self.write_throughput_per_thread, self.receiver_buffer_in_use)\n",
    "            throughput_increase = min(write_throughput_temp, self.write_bandwidth-self.write_throughput)\n",
    "            self.write_throughput += throughput_increase\n",
    "            self.receiver_buffer_in_use -= throughput_increase\n",
    "\n",
    "        time_taken = throughput_increase / self.write_throughput_per_thread\n",
    "        next_time = time + time_taken + 0.001\n",
    "        if next_time < 1:\n",
    "            self.thread_queue.put((next_time, \"write\"))\n",
    "        # print(f\"Write Thread: Sender Buffer: {self.sender_buffer_in_use}, Receiver Buffer: {self.receiver_buffer_in_use}\")\n",
    "        if throughput_increase > 0 and self.track_states:\n",
    "            with open('thread_level_states.csv', 'a') as f:\n",
    "                f.write(f\"Write, {throughput_increase}, {self.sender_buffer_in_use}, {self.receiver_buffer_in_use}\\n\")\n",
    "        return next_time\n",
    "\n",
    "    def get_utility_value_dummy(self, threads):\n",
    "        x1, x2, x3 = map(int, threads)\n",
    "        return ((x1 - 1) ** 2 + (x2 - 2) ** 2 + (x3 + 3) ** 2 + \\\n",
    "            np.sin(2 * x1) + np.sin(2 * x2) + np.cos(2 * x3)) * -1\n",
    "\n",
    "    def get_utility_value(self, threads):\n",
    "        read_thread, network_thread, write_thread = map(int, threads)\n",
    "        self.read_thread = read_thread\n",
    "        self.network_thread = network_thread\n",
    "        self.write_thread = write_thread\n",
    "\n",
    "        self.thread_queue = PriorityQueue() # Key: time, Value: thread_type\n",
    "        self.read_throughput = 0\n",
    "        self.network_throughput = 0\n",
    "        self.write_throughput = 0\n",
    "\n",
    "        # populate the thread queue\n",
    "        for i in range(read_thread):\n",
    "            self.thread_queue.put((0, \"read\"))\n",
    "        for i in range(network_thread):\n",
    "            self.thread_queue.put((0, \"network\"))\n",
    "        for i in range(write_thread):\n",
    "            self.thread_queue.put((0, \"write\"))\n",
    "\n",
    "        read_thread_finish_time = 0\n",
    "        network_thread_finish_time = 0\n",
    "        write_thread_finish_time = 0\n",
    "\n",
    "        while not self.thread_queue.empty():\n",
    "            time, thread_type = self.thread_queue.get()\n",
    "            if thread_type == \"read\":\n",
    "                read_thread_finish_time = self.read_thread_task(time)\n",
    "            elif thread_type == \"network\":\n",
    "                network_thread_finish_time = self.network_thread_task(time)\n",
    "            elif thread_type == \"write\":\n",
    "                write_thread_finish_time = self.write_thread_task(time)\n",
    "\n",
    "        self.read_throughput = self.read_throughput / read_thread_finish_time\n",
    "        self.network_throughput = self.network_throughput / network_thread_finish_time\n",
    "        self.write_throughput = self.write_throughput / write_thread_finish_time\n",
    "\n",
    "        self.sender_buffer_in_use = max(self.sender_buffer_in_use, 0)\n",
    "        self.receiver_buffer_in_use = max(self.receiver_buffer_in_use, 0)\n",
    "\n",
    "        utility = (self.read_throughput/self.K ** read_thread) + (self.network_throughput/self.K ** network_thread) + (self.write_throughput/self.K ** write_thread)\n",
    "\n",
    "        # print(f\"Read thread: {read_thread}, Network thread: {network_thread}, Write thread: {write_thread}, Utility: {utility}\")\n",
    "\n",
    "        if self.track_states:\n",
    "            with open('optimizer_call_level_states.csv', 'a') as f:\n",
    "                f.write(f\"{read_thread}, {network_thread}, {write_thread}, {utility}, {self.read_throughput}, {self.sender_buffer_in_use}, {self.network_throughput}, {self.receiver_buffer_in_use}, {self.write_throughput}\\n\")\n",
    "\n",
    "        final_state = SimulatorState(self.sender_buffer_capacity-self.sender_buffer_in_use,\n",
    "                                     self.receiver_buffer_capacity-self.receiver_buffer_in_use,\n",
    "                                     self.read_throughput, self.write_throughput, self.network_throughput,\n",
    "                                     read_thread, write_thread, network_thread)\n",
    "\n",
    "        return utility, final_state"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T22:12:26.684886Z",
     "start_time": "2024-12-04T22:12:26.679418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NetworkOptimizationEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(NetworkOptimizationEnv, self).__init__()\n",
    "        oneGB = 1024\n",
    "        self.simulator = NetworkSystemSimulator(sender_buffer_capacity=5*oneGB,\n",
    "                                                receiver_buffer_capacity=3*oneGB,\n",
    "                                                read_throughput_per_thread=100,\n",
    "                                                network_throughput_per_thread=75,\n",
    "                                                write_throughput_per_thread=35,\n",
    "                                                read_bandwidth=6*oneGB,\n",
    "                                                write_bandwidth=700,\n",
    "                                                network_bandwidth=1*oneGB)\n",
    "        self.thread_limits = [1, 100]  # Threads can be between 1 and 10\n",
    "\n",
    "        # Continuous action space: adjustments between -5.0 and +5.0\n",
    "        self.action_space = spaces.Box(low=np.array([-5.0, -5.0, -5.0]),\n",
    "                                       high=np.array([5.0, 5.0, 5.0]),\n",
    "                                       dtype=np.float32)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0, 0, 0, 0, 0, self.thread_limits[0], self.thread_limits[0], self.thread_limits[0]]),\n",
    "            high=np.array([\n",
    "                self.simulator.sender_buffer_capacity,\n",
    "                self.simulator.receiver_buffer_capacity,\n",
    "                np.inf,  # Or maximum possible throughput values\n",
    "                np.inf,\n",
    "                np.inf,\n",
    "                self.thread_limits[1],\n",
    "                self.thread_limits[1],\n",
    "                self.thread_limits[1]\n",
    "            ]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.state = SimulatorState(sender_buffer_remaining_capacity=5*oneGB,\n",
    "                                    receiver_buffer_remaining_capacity=3*oneGB,\n",
    "                                    read_thread=1,\n",
    "                                    network_thread=1,\n",
    "                                    write_thread=1)\n",
    "        self.max_steps = 100\n",
    "        self.current_step = 0\n",
    "\n",
    "        # For recording the trajectory\n",
    "        self.trajectory = []\n",
    "\n",
    "    def step(self, action):\n",
    "        adjustment = action\n",
    "\n",
    "        # Update the thread counts\n",
    "        new_thread_counts = [np.round(self.state.read_thread + adjustment[0]),\n",
    "                            np.round(self.state.network_thread + adjustment[1]),\n",
    "                            np.round(self.state.write_thread + adjustment[2])]\n",
    "\n",
    "        # Ensure the thread counts are within limits\n",
    "        new_thread_counts = np.clip(new_thread_counts, self.thread_limits[0], self.thread_limits[1]).astype(np.int32)\n",
    "\n",
    "        # Compute utility and update state\n",
    "        utility, self.state = self.simulator.get_utility_value(new_thread_counts)\n",
    "        reward = utility\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.max_steps\n",
    "\n",
    "        # Record the state\n",
    "        self.trajectory.append(self.state.copy())\n",
    "\n",
    "        # Return state as NumPy array\n",
    "        return self.state.to_array(), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        oneGB = 1024\n",
    "        self.state = SimulatorState(sender_buffer_remaining_capacity=5*oneGB,\n",
    "                                    receiver_buffer_remaining_capacity=3*oneGB,\n",
    "                                    read_thread=1,\n",
    "                                    network_thread=1,\n",
    "                                    write_thread=1)\n",
    "        self.current_step = 0\n",
    "        self.simulator.sender_buffer_in_use = 0\n",
    "        self.simulator.receiver_buffer_in_use = 0\n",
    "        self.trajectory = [self.state.copy()]\n",
    "\n",
    "        # Return initial state as NumPy array\n",
    "        return self.state.to_array()\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T22:12:26.732789Z",
     "start_time": "2024-12-04T22:12:26.729349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PolicyNetworkContinuous(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNetworkContinuous, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mean_layer = nn.Linear(128, action_dim)\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))  # Log standard deviation as a parameter\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.fc(state)\n",
    "        mean = self.mean_layer(x)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return mean, std"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T22:12:26.780292Z",
     "start_time": "2024-12-04T22:12:26.777063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)  # Single output for value\n",
    "        )\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.fc(state)\n",
    "        return value"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T22:12:26.835998Z",
     "start_time": "2024-12-04T22:12:26.830154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PPOAgentContinuous:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, eps_clip=0.2):\n",
    "        self.policy = PolicyNetworkContinuous(state_dim, action_dim)\n",
    "        self.policy_old = PolicyNetworkContinuous(state_dim, action_dim)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        self.value_function = ValueNetwork(state_dim)\n",
    "        self.optimizer = optim.Adam([\n",
    "            {'params': self.policy.parameters(), 'lr': lr},\n",
    "            {'params': self.value_function.parameters(), 'lr': lr}\n",
    "        ])\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        mean, std = self.policy_old(state)\n",
    "        dist = Normal(mean, std)\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        return action.detach().cpu().numpy(), action_logprob.detach().cpu().numpy()\n",
    "\n",
    "    def update(self, memory):\n",
    "        states = torch.stack(memory.states).to(device)\n",
    "        actions = torch.tensor(np.array(memory.actions), dtype=torch.float32).to(device)\n",
    "        rewards = torch.tensor(memory.rewards, dtype=torch.float32).to(device)\n",
    "        old_logprobs = torch.tensor(np.array(memory.logprobs), dtype=torch.float32).to(device)\n",
    "\n",
    "        # Compute discounted rewards\n",
    "        returns = []\n",
    "        discounted_reward = 0\n",
    "        for reward in reversed(rewards):\n",
    "            discounted_reward = reward + self.gamma * discounted_reward\n",
    "            returns.insert(0, discounted_reward)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32).to(device)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-5)\n",
    "\n",
    "        # Get new action probabilities\n",
    "        mean, std = self.policy(states)\n",
    "        dist = Normal(mean, std)\n",
    "        logprobs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy()\n",
    "\n",
    "        logprobs = logprobs.sum(dim=1)\n",
    "        old_logprobs = old_logprobs.sum(dim=1)\n",
    "        entropy = entropy.sum(dim=1)\n",
    "\n",
    "        ratios = torch.exp(logprobs - old_logprobs)\n",
    "        state_values = self.value_function(states).squeeze()\n",
    "\n",
    "        # Compute advantage\n",
    "        advantages = returns - state_values.detach()\n",
    "\n",
    "        # Surrogate loss\n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "        loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, returns) - 0.01 * entropy\n",
    "\n",
    "        # Update policy\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T22:12:26.884271Z",
     "start_time": "2024-12-04T22:12:26.881530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def clear(self):\n",
    "        del self.states[:]\n",
    "        del self.actions[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T22:12:26.933876Z",
     "start_time": "2024-12-04T22:12:26.929036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_ppo(env, agent, max_episodes=1000):\n",
    "    memory = Memory()\n",
    "    total_rewards = []\n",
    "    for episode in tqdm(range(1, max_episodes + 1), desc=\"Episodes\"):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        for t in range(env.max_steps):\n",
    "            action, action_logprob = agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            memory.states.append(torch.FloatTensor(state).to(device))\n",
    "            memory.actions.append(action)\n",
    "            memory.logprobs.append(action_logprob)\n",
    "            memory.rewards.append(reward)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        agent.update(memory)\n",
    "\n",
    "        # print(f\"Episode {episode}\\tLast State: {state}\\tReward: {reward}\")\n",
    "        with open('episode_rewards_twohl_ReLU_GPU_faster.csv', 'a') as f:\n",
    "                f.write(f\"Episode {episode}, Last State: {np.round(state[-3:])}, Reward: {reward}\\n\")\n",
    "\n",
    "        memory.clear()\n",
    "        total_rewards.append(episode_reward)\n",
    "        if episode % 10 == 0:\n",
    "            avg_reward = np.mean(total_rewards[-10:])\n",
    "            print(f\"Episode {episode}\\tAverage Reward: {avg_reward:.2f}\")\n",
    "        if episode % 10 == 0:\n",
    "            save_model(agent, \"models/twohl_ReLU_GPU_faster_policy_\"+ str(episode) +\".pth\", \"models/twohl_ReLU_GPU_faster_value_\"+ str(episode) +\".pth\")\n",
    "            print(\"Model saved successfully.\")\n",
    "    return total_rewards\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T22:12:26.984713Z",
     "start_time": "2024-12-04T22:12:26.982223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_trajectory(trajectory):\n",
    "    trajectory = np.array(trajectory)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(trajectory[:, 0], label='Read Thread')\n",
    "    plt.plot(trajectory[:, 1], label='Network Thread')\n",
    "    plt.plot(trajectory[:, 2], label='Write Thread')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Number of Threads')\n",
    "    plt.title('Trajectory of Thread Counts During Last Episode')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_rewards(rewards, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-04T22:12:27.025657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = NetworkOptimizationEnv()\n",
    "agent = PPOAgentContinuous(state_dim=8, action_dim=3)\n",
    "print(\"Training PPO agent on Network System Simulator with continuous actions...\")\n",
    "rewards = train_ppo(env, agent, max_episodes=1000)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Sender Buffer: 25, Receiver Buffer: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rs75c/Falcon-File-Transfer-Optimizer/venv/lib/python3.12/site-packages/gym/spaces/box.py:127: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PPO agent on Network System Simulator with continuous actions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes:   1%|          | 10/1000 [03:08<4:38:56, 16.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage Reward: 7653.10\n",
      "Model saved successfully.\n",
      "Model saved successfully.\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plot_trajectory(env.trajectory)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "plot_rewards(rewards, 'PPO Training Rewards')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
